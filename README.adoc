= Low-Latency Performance Workshop for OpenShift 4.20

== Introduction

This repository hosts the comprehensive lab guide for the **Low-Latency Performance Workshop**, designed to teach OpenShift 4.20 performance optimization through hands-on exercises using modern tooling and GitOps practices.

The workshop can be delivered by Red Hat Associates, partners, and community members to customers and interested parties worldwide.

It is designed to work with the Showroom lab environment provided by the Red Hat Demo Platform (RHDP) team, as well as customer environments and self-paced learning scenarios.

This workshop provides hands-on experience with **low-latency performance optimization** in OpenShift 4.20, covering everything from baseline performance testing to advanced virtualization optimization.

**Low-latency performance optimization** enables organizations to achieve deterministic, predictable performance for time-sensitive workloads such as financial trading systems, industrial automation, telecommunications, and real-time analytics.

In this workshop, participants will explore comprehensive performance optimization techniques that platform engineers, SREs, and performance specialists encounter when building and maintaining high-performance OpenShift environments.

== Who Will Benefit Most from This Workshop?

**Platform Engineers & SREs** -- Those responsible for designing, deploying, and maintaining OpenShift clusters that require predictable, low-latency performance. These professionals need to understand CPU isolation, memory optimization, and performance monitoring to meet strict SLA requirements.

**Performance Specialists** -- Those responsible for optimizing application performance and conducting performance testing in containerized and virtualized environments. These users need expertise in performance profiling, bottleneck identification, and optimization validation.

**DevOps Engineers** -- Those implementing GitOps workflows for performance-critical applications and infrastructure. These professionals need to understand how to deploy and manage performance optimizations through automated pipelines.

**Solutions Architects** -- Those designing OpenShift solutions for performance-sensitive use cases. These users need to understand the trade-offs and capabilities of different performance optimization approaches.
== What Content Is Covered In The Workshop?

The workshop consists of **8 comprehensive modules** that build upon each other to provide complete low-latency performance optimization expertise:

**Module 1: Low-Latency Performance Fundamentals** -- Introduction to low-latency concepts, OpenShift 4.20 performance features, workshop architecture, and prerequisites. Participants learn the theoretical foundation and prepare their environment.

**Module 2: Environment Setup and Verification** -- Hands-on verification of the pre-configured Single Node OpenShift (SNO) environment. Participants verify installed operators (OpenShift Virtualization, SR-IOV, Node Tuning Operator) and understand their purpose.

**Module 3: Baseline Performance Testing** -- Comprehensive performance baseline establishment using kube-burner v1.17+. Participants learn modern performance testing methodologies and establish measurable baselines for optimization comparison.

**Module 4: Core Performance Tuning** -- Deep dive into Performance Profiles, CPU isolation, HugePages configuration, and real-time kernel optimization. Participants apply and validate core performance optimizations.

**Module 5: Low-Latency Virtualization** -- Advanced virtual machine optimization using OpenShift Virtualization, including VMI latency testing, SR-IOV networking, and network policy performance analysis. Participants optimize VMs for low-latency workloads.

**Module 6: Monitoring and Validation** -- Production-ready monitoring setup with Prometheus and Grafana, comprehensive performance validation across all optimization types, and continuous performance testing implementation. Participants establish sustainable performance assurance practices.

**Module 7: GPU Workloads (Optional)** -- Advanced module covering GPU Operator installation, GPU-enabled workload deployment, and GPU performance testing on OpenShift.

**Module 8: Workshop Conclusion and Next Steps** -- Summary of key learnings, best practices, troubleshooting guidance, and recommendations for further exploration.
== Technology Stack and Version Information

This edition of the **Low-Latency Performance Workshop** has been developed and tested with the following software versions:

* **OpenShift Container Platform**: 4.20+
* **OpenShift Virtualization**: 4.20+
* **SR-IOV Network Operator**: 4.20+
* **Node Tuning Operator**: Built-in to OpenShift 4.20+
* **Cert Manager**: 1.18+ (for SSL certificate management)
* **kube-burner**: 1.17+ (performance testing tool)
* **Prometheus**: 2.45+ (monitoring and metrics)
* **Grafana**: 10.0+ (visualization and dashboards)

== Workshop Architecture

The workshop uses a **Hub + Spoke architecture** to ensure workshop documentation remains available during student cluster reboots:

=== Hub + Spoke Design

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           Hub Cluster                                    │
│                     (workshop-hub-aws.yml)                               │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐          │
│  │ student1-workshop│  │ student2-workshop│  │ student3-workshop│         │
│  │   (Showroom)    │  │   (Showroom)    │  │   (Showroom)    │          │
│  │  + Wetty Term   │  │  + Wetty Term   │  │  + Wetty Term   │          │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘          │
│           │                    │                    │                    │
└───────────┼────────────────────┼────────────────────┼────────────────────┘
            │ SSH                │ SSH                │ SSH
            ▼                    ▼                    ▼
┌───────────────────┐  ┌───────────────────┐  ┌───────────────────┐
│  student1 SNO     │  │  student2 SNO     │  │  student3 SNO     │
│  (Spoke Cluster)  │  │  (Spoke Cluster)  │  │  (Spoke Cluster)  │
│  - Performance    │  │  - Performance    │  │  - Performance    │
│  - Virtualization │  │  - Virtualization │  │  - Virtualization │
└───────────────────┘  └───────────────────┘  └───────────────────┘
```

=== Why Hub + Spoke?

When students apply Performance Profiles in Module 4, their SNO cluster **reboots**. If Showroom runs on the same cluster, students lose access to instructions. The Hub + Spoke architecture solves this:

* **Hub cluster** - Stable, never reboots, hosts all workshop documentation (Showroom)
* **Student SNO clusters** - Can reboot safely during hands-on performance tuning
* **Per-student Showrooms** - Each student gets their own Showroom instance with embedded terminal
* **Students always have access** to instructions, even during cluster reboots

=== Components

* **Hub Cluster**: Standard OpenShift cluster hosting Showroom for all students
* **Student SNO Clusters**: Single Node OpenShift clusters for hands-on performance tuning
* **Per-Student Showrooms**: Individual workshop instances with Wetty terminals connected to each student's bastion
* **Performance Testing**: Modern kube-burner configurations with structured metrics
* **Environment**: AWS-based with `*.opentlc.com` domain pattern

== Key Learning Outcomes

Upon completion of this workshop, participants will be able to:

* **Establish performance baselines** using modern testing methodologies
* **Apply CPU isolation and HugePages** for deterministic performance
* **Optimize virtual machines** for low-latency workloads
* **Implement comprehensive monitoring** for performance assurance
* **Deploy optimizations via GitOps** for reproducible environments
* **Validate performance improvements** with quantitative measurements
* **Troubleshoot performance issues** using systematic approaches

== Repository Structure

```
low-latency-performance-workshop/
├── content/                          # Workshop documentation
│   ├── modules/ROOT/pages/          # Module content (AsciiDoc)
│   ├── antora.yml                   # Antora configuration
│   └── lib/                         # Extensions and utilities
├── agnosticd-v2-vars/               # AgnosticD v2 configurations
│   ├── low-latency-sno-aws.yml      # Student SNO cluster config
│   ├── workshop-hub-aws.yml         # Hub cluster config (hosts Showroom)
│   └── README.md                    # Detailed deployment documentation
├── scripts/                         # Deployment scripts
│   ├── workshop-setup.sh            # Initial environment setup
│   ├── deploy-workshop.sh           # Automated full deployment (Hub + Students)
│   ├── deploy-student-showrooms.sh  # Per-student Showroom deployment
│   ├── deploy-sno.sh                # Deploy single SNO cluster
│   ├── destroy-sno.sh               # Destroy SNO cluster
│   ├── status-sno.sh                # Check cluster status
│   └── delete-vpc.sh                # Idempotent VPC deletion script
├── gitops/                          # GitOps configurations
│   ├── kube-burner-configs/         # Performance testing configs
│   ├── openshift-virtualization/    # CNV deployment configs
│   └── sriov-network-operator/      # SR-IOV deployment configs
├── docs/                            # Additional documentation
│   ├── WORKSHOP_SETUP.md            # Setup guide
│   └── adr/                         # Architecture Decision Records
├── DEVELOPER_GUIDE.md               # Developer and instructor guide
├── ENVIRONMENT_CONFIG.md            # Environment configuration reference
└── README.adoc                      # This file
```

== Getting Started

=== For Workshop Participants

1. **Access your workshop environment** using the provided GUID
2. **SSH to your bastion host**: `ssh ec2-user@bastion.{guid}.dynamic.redhatworkshops.io`
3. **Follow the workshop modules** starting with Module 1
4. **Complete hands-on exercises** in each module
5. **Validate your progress** using provided testing tools

=== For Workshop Instructors

1. **Review the setup guide**: See link:docs/WORKSHOP_SETUP.md[Workshop Setup Guide]
2. **Review the DEVELOPER_GUIDE.md** for comprehensive setup instructions
3. **Configure workshop environment** using ENVIRONMENT_CONFIG.md
4. **Validate environment setup** using provided validation scripts
5. **Customize content** for your specific audience if needed
6. **Deploy workshop infrastructure** using the deployment scripts

=== For Contributors

1. **Fork the repository** and create a feature branch
2. **Follow AsciiDoc style guidelines** in DEVELOPER_GUIDE.md
3. **Test changes** with actual workshop environment
4. **Submit pull requests** with clear descriptions
5. **Update documentation** as needed

== Workshop Deployment (Quick Start)

=== Step 1: Initial Setup (5 minutes)

[source,bash]
----
# Clone the repository
git clone https://github.com/tosin2013/low-latency-performance-workshop.git
cd low-latency-performance-workshop

# Run the setup script
./scripts/workshop-setup.sh
----

=== Step 2: Configure Secrets

Edit the secrets files created in `~/Development/agnosticd-v2-secrets/`:

[source,bash]
----
# Add your OpenShift pull secret
vim ~/Development/agnosticd-v2-secrets/secrets.yml

# Add AWS credentials (rename XXX to your sandbox number)
cp ~/Development/agnosticd-v2-secrets/secrets-sandboxXXX.yml \
   ~/Development/agnosticd-v2-secrets/secrets-sandbox5466.yml
vim ~/Development/agnosticd-v2-secrets/secrets-sandbox5466.yml
----

=== Step 3: Deploy Full Workshop (Automated)

Use the automated deployment script to deploy Hub + Student clusters:

[source,bash]
----
./scripts/deploy-workshop.sh \
  --hub-account sandbox5466 \
  --student-account sandbox5466 \
  --students student1,student2
----

This script will:

1. Deploy student SNO clusters (one per student)
2. Collect credentials from each deployment
3. Deploy Hub cluster with Showroom
4. Deploy per-student Showroom instances

=== Step 4: Access the Workshop

After deployment, each student gets their own workshop URL:

[cols="1,2"]
|===
| Student | Workshop URL

| student1
| `https://student1-workshop-low-latency-workshop.apps.ocp.hub.sandbox5466.opentlc.com/`

| student2
| `https://student2-workshop-low-latency-workshop.apps.ocp.hub.sandbox5466.opentlc.com/`
|===

Each student's Showroom includes:

* Workshop documentation (left panel)
* Terminal connected to their bastion (right panel)

=== Manual Deployment (Alternative)

If you prefer to deploy manually:

[source,bash]
----
cd ~/Development/agnosticd-v2

# 1. Deploy student SNO clusters FIRST
./bin/agd provision -g student1 -c low-latency-sno-aws -a sandbox5466
./bin/agd provision -g student2 -c low-latency-sno-aws -a sandbox5466

# 2. Deploy Hub cluster
./bin/agd provision -g hub -c workshop-hub-aws -a sandbox5466

# 3. Deploy per-student Showroom instances
cd ~/low-latency-performance-workshop
./scripts/deploy-student-showrooms.sh --students student1,student2
----

=== Check Status

[source,bash]
----
./scripts/status-sno.sh student1 sandbox5466
----

=== Destroy Clusters

[source,bash]
----
cd ~/Development/agnosticd-v2
./bin/agd destroy -g student1 -c low-latency-sno-aws -a sandbox5466
./bin/agd destroy -g hub -c workshop-hub-aws -a sandbox5466
----

For complete setup instructions, see link:docs/WORKSHOP_SETUP.md[Workshop Setup Guide] and link:agnosticd-v2-vars/README.md[AgnosticD Configuration Guide].

=== After Deployment

[source,bash]
----
# Set kubeconfig
export KUBECONFIG=~/Development/agnosticd-v2-output/student1/openshift-cluster_student1_kubeconfig

# Check SNO cluster status
oc get nodes

# Find deployment outputs
ls ~/Development/agnosticd-v2-output/student1/
----

=== Expected Output

After successful deployment, each user will have:

**Per-User Resources:**
[cols="1,2"]
|===
| Resource | Description

| SNO Cluster
| Single Node OpenShift on AWS

| Bastion
| t3a.medium instance for cluster management

| Showroom
| Workshop documentation site
|===

**Output Files (per user):**
----
~/Development/agnosticd-v2-output/studentX/
├── openshift-cluster_studentX_kubeconfig           # SNO cluster access
├── openshift-cluster_studentX_kubeadmin-password   # Admin password
└── provision-user-info.yaml                        # User connection info
----

**Installed Operators (on each SNO):**

* OpenShift Virtualization
* Node Tuning Operator (built-in)

**Estimated Deployment Time:**

* Single SNO cluster: ~45-60 minutes

**Deployment Documentation:**

* **Setup Guide**: link:docs/WORKSHOP_SETUP.md[Complete Setup Guide]
* **Deployment README**: link:DEPLOYMENT-README.md[Deployment README]

== Support and Community

* **GitHub Issues**: Report bugs and request features
* **Documentation**: Comprehensive guides in DEVELOPER_GUIDE.md
* **Community**: Engage with OpenShift and performance optimization communities
* **Red Hat Support**: Available for Red Hat customers and partners

== License

This workshop content is provided under the Apache 2.0 license. See LICENSE file for details.

---

**Workshop Version**: OpenShift 4.20
**Last Updated**: January 2026
**Maintained by**: Red Hat RTO