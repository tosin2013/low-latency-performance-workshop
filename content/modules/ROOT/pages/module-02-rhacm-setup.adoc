= Module 2: Workshop Setup with RHACM Multi-Cluster Management

[%hardbreaks]

[IMPORTANT]
====
*Quick Start: Login to Your SNO Cluster*

For RHPDS deployments, your SNO cluster has a pre-configured user account with cluster-admin permissions:

[cols="1,2"]
|===
| API URL | `https://api.workshop-{username}.{subdomain}:6443`
| Console | `https://console-openshift-console.apps.workshop-{username}.{subdomain}`
| Username | `{username}` (same as your hub username, e.g., `user1`)
| Password | Provided by workshop administrator
|===

*Login to your SNO cluster (replace values for your environment):*

[source,bash,role=execute]
----
# Replace <username>, <subdomain>, and <password> with your actual values
oc login https://api.workshop-<username>.<subdomain>:6443 -u <username> -p <password>
----

Replace:

* `<username>` with your assigned username (e.g., user1, user2, etc.)
* `<subdomain>` with your cluster's subdomain (provided by admin, e.g., `abc123.sandbox.opentlc.com`)
* `<password>` with the password provided by your administrator

TIP: Your workshop administrator will provide the specific subdomain and password for your environment.
====

[TIP]
====
*Launch Your Development Environment*

Click below to launch your pre-configured Dev Spaces workspace with all tools and credentials:

link:{devspaces_factory}[ðŸš€ *Open in Dev Spaces*,role=button]

Your workspace includes:

* `oc` CLI pre-configured with your SNO kubeconfig
* SSH keys for bastion access
* `kube-burner` for performance testing
* Workshop documentation and scripts
====

== Module Overview

This module covers the workshop infrastructure using Red Hat Advanced Cluster Management (RHACM). Your SNO cluster comes **pre-configured** with all required operators, so this module focuses on understanding the architecture and verifying your environment is ready.

[NOTE]
====
**Workshop Environment Pre-Configuration**

Your Single Node OpenShift (SNO) cluster has been automatically provisioned with:

* âœ… **OpenShift Virtualization** - Deployed and configured
* âœ… **SR-IOV Network Operator** - Deployed and configured
* âœ… **Node Tuning Operator** - Built-in (OpenShift 4.11+)
* âœ… **RHACM Integration** - Cluster imported to hub

This module teaches you **how** these components were deployed using GitOps, which is valuable knowledge for your own environments.
====

== Key Learning Objectives

* Understand the workshop architecture and safety approach
* Learn RHACM cluster management concepts
* **Verify** your pre-configured target cluster in RHACM
* Understand multi-cluster GitOps workflows (reference examples provided)
* Confirm the foundation for performance testing and tuning

== Workshop Architecture and Safety Approach

.*Why Multi-Cluster Management?*
This workshop uses a multi-cluster architecture for safety and realism:

* *Hub Cluster*: Management cluster running RHACM (where you'll execute commands)
* *Target Cluster*: Single Node OpenShift (SNO) where performance tuning is applied
* *Safety First*: Isolates performance tuning from the management environment
* *Real-World Pattern*: Mirrors enterprise multi-cluster management practices

.*Benefits of This Approach*
* *Isolation*: Performance tuning won't affect your primary workshop environment
* *Repeatability*: Fresh target cluster for each workshop run  
* *Enterprise Relevance*: Learn real-world multi-cluster management patterns
* *Risk Mitigation*: Node reboots and kernel changes won't disrupt the workshop

== Prerequisites

Before starting this module, ensure you have:

* Access to the hub cluster with RHACM installed
* A target cluster imported into RHACM (Single Node OpenShift recommended)
* Basic understanding of GitOps principles
* Familiarity with OpenShift and Kubernetes concepts

== RHACM Architecture Overview

=== Core Components

[cols="1,3"]
|===
| Component | Description

| Hub Cluster
| Central management cluster running RHACM that manages multiple spoke clusters

| Managed Clusters
| Remote OpenShift clusters managed by the hub cluster

| ManagedClusterSet
| Logical grouping of managed clusters for policy and application deployment

| Placement
| Defines which clusters should receive specific applications or policies

| GitOpsCluster
| Integration point between RHACM and ArgoCD for multi-cluster GitOps
|===

=== Multi-Cluster Benefits

* *Centralized Management*: Single pane of glass for multiple clusters
* *GitOps Workflows*: Declarative application deployment across clusters
* *Policy Enforcement*: Consistent security and compliance across environments
* *Disaster Recovery*: Application portability between clusters

== Hands-on Exercise: Verifying Your Pre-Configured Workshop Environment

=== Exercise Overview

Your SNO cluster has been automatically configured with all required operators. In this exercise, you will:

1. **Verify** your managed cluster is imported in RHACM
2. **Confirm** the pre-installed operators are running
3. **Understand** how GitOps deployment works (reference examples)
4. **Explore** the RHACM-ArgoCD integration architecture

=== Step 1: Verify Your Managed Cluster in RHACM

Your SNO cluster has been automatically imported into RHACM. Let's verify.

. Log into the hub cluster using the provided credentials:
+
[source,bash,role=execute]
----
oc login --token=<hub-cluster-token> --server=<hub-cluster-api>
----

. List the managed clusters (your SNO should appear as `workshop-<username>`):
+
[source,bash,role=execute]
----
oc get managedclusters
----
+
You should see output similar to:
+
[source,bash]
----
NAME                HUB ACCEPTED   MANAGED CLUSTER URLS                                        JOINED   AVAILABLE   AGE
local-cluster       true           https://api.hub-cluster.sandbox.opentlc.com:6443            True     True        4h40m
workshop-student1   true           https://api.test-student1.sandbox.opentlc.com:6443          True     True        45m
----

. Verify your cluster is AVAILABLE:
+
[source,bash,role=execute]
----
oc get managedclusters -o wide
----

. Set your cluster name as an environment variable:
+
[source,bash,role=execute]
----
export MANAGED_CLUSTER=workshop-student1  # Replace with your cluster name
----

=== Step 2: Verify Pre-Installed Operators on Your SNO

Your SNO cluster was provisioned with all required operators. Let's verify they're running.

. Get your SNO kubeconfig (provided during workshop setup):
+
[source,bash,role=execute]
----
# Your kubeconfig is at: ~/agnosticd-output/<guid>/kubeconfig
export KUBECONFIG=~/agnosticd-output/test-student1/low-latency-workshop-sno_test-student1_kubeconfig
----

. Verify OpenShift Virtualization is installed:
+
[source,bash,role=execute]
----
oc get csv -n openshift-cnv
----
+
Expected output:
+
[source,bash]
----
NAME                                      DISPLAY                    VERSION   PHASE
kubevirt-hyperconverged-operator.v4.x.x   OpenShift Virtualization   4.x.x     Succeeded
----

. Verify SR-IOV Network Operator is installed:
+
[source,bash,role=execute]
----
oc get csv -n openshift-sriov-network-operator
----
+
Expected output:
+
[source,bash]
----
NAME                                         DISPLAY                  VERSION   PHASE
sriov-network-operator.v4.x.x                SR-IOV Network Operator  4.x.x     Succeeded
----

. Verify the built-in Node Tuning Operator (available in all OpenShift 4.11+ clusters):
+
[source,bash,role=execute]
----
# Check Node Tuning Operator pods
oc get pods -n openshift-cluster-node-tuning-operator

# Verify Performance Profile CRD is available
oc get crd performanceprofiles.performance.openshift.io
----

. Check the HyperConverged instance (OpenShift Virtualization):
+
[source,bash,role=execute]
----
oc get hyperconverged -n openshift-cnv
----

[NOTE]
====
**All operators should show as "Succeeded" or "Running"**

If any operator is not yet ready, wait a few minutes - the provisioning script deploys them automatically and they may still be initializing.

**What was deployed automatically:**

* **OpenShift Virtualization**: Namespace `openshift-cnv`, Subscription, OperatorGroup, HyperConverged instance
* **SR-IOV Network Operator**: Namespace `openshift-sriov-network-operator`, Subscription, OperatorGroup
* **Node Tuning Operator**: Built-in to OpenShift (no installation needed)
====

=== Step 3: Understanding the GitOps Architecture (Reference)

[NOTE]
====
**This section is for reference only** - Your workshop environment is already configured. This explains *how* the operators were deployed, which is valuable knowledge for your own environments.
====

The workshop repository includes pre-configured resources for RHACM-ArgoCD integration.

. Review the rhacm-argocd-integration resources:
+
[source,bash,role=execute]
----
cd /home/ec2-user/low-latency-performance-workshop/rhacm-argocd-integration
ls -la
cat README.md
----
+
The directory contains:
+
* `managedclusterset.yaml` - Groups managed clusters logically
* `managedclustersetbinding.yaml` - Binds cluster set to openshift-gitops namespace
* `placement.yaml` - Defines cluster selection criteria
* `gitopscluster.yaml` - Integrates RHACM with ArgoCD
* `kustomization.yaml` - Kustomize configuration for all resources

. **[Reference]** These resources would be applied with:
+
[source,bash]
----
# Already done during workshop provisioning
oc apply -k .
----

. Check the existing RHACM-ArgoCD integration:
+
[source,bash,role=execute]
----
# Check ManagedClusterSet
oc get managedclusterset all-clusters

# Check placement decisions
oc get placementdecision -n openshift-gitops

# Verify clusters are available in ArgoCD
oc get secrets -n openshift-gitops | grep cluster
----

== Reference: GitOps Deployment with ArgoCD (How Your Environment Was Configured)

[NOTE]
====
**This entire section is reference material** showing how operators were deployed to your SNO cluster. Understanding these patterns is valuable for deploying to your own environments.

**Your workshop SNO was configured using direct Ansible automation** (faster and simpler for workshops), but the GitOps approach shown here is the enterprise pattern for production environments.
====

=== Understanding the ArgoCD Application Structure

The workshop repository includes ArgoCD applications that could be used to deploy operators via GitOps.

. Review the argocd-apps directory:
+
[source,bash,role=execute]
----
cd /home/ec2-user/low-latency-performance-workshop/argocd-apps
ls -la
cat README.md
----

. Examine an ArgoCD Application manifest:
+
[source,bash,role=execute]
----
cat openshift-virtualization-operator.yaml
----
+
Key components of an ArgoCD Application:
+
* `spec.source.repoURL` - Git repository containing the manifests
* `spec.source.path` - Path within the repo to the Kustomize overlay
* `spec.destination.server` - Target cluster API URL
* `spec.syncPolicy` - Automatic sync with prune and self-heal

=== GitOps Deployment Pattern (Reference)

[IMPORTANT]
====
**For Your Own Environments:** If you want to use GitOps to deploy operators, you would:

1. Update destination server URLs to match your cluster
2. Apply the ArgoCD applications to your hub cluster
3. ArgoCD would then sync the operators to your target clusters
====

. **[Reference]** Update destination server URLs for your cluster:
+
[source,bash]
----
# Get your cluster's API server URL
CLUSTER_API_URL=$(oc whoami --show-server)

# Update the destination server in ArgoCD application files using yq
for file in sriov-network-operator.yaml openshift-virtualization-operator.yaml openshift-virtualization-instance.yaml; do
  yq eval ".spec.destination.server = \"$CLUSTER_API_URL\"" -i "$file"
done
----

. **[Reference]** Deploy via ArgoCD:
+
[source,bash]
----
# On the hub cluster where ArgoCD is running
oc apply -k .
----

[NOTE]
====
**OpenShift 4.19+ Performance Operator Architecture**

* **Node Tuning Operator**: Built-in to OpenShift 4.11+ (no installation required)
* **Performance Addon Operator**: **DEPRECATED** in 4.11+ (functionality moved to Node Tuning Operator)
* **SR-IOV Network Operator**: Requires installation for high-performance networking
* **OpenShift Virtualization**: Requires installation for VM scenarios

For more details, see: https://docs.openshift.io/container-platform/4.19/scalability_and_performance/cnf-low-latency-tuning.html[OpenShift 4.19 Low Latency Tuning Documentation^]
====

=== Step 4: Verify Your Environment (Continue Here)

Now let's verify your pre-configured environment is ready. Ensure you're on your SNO cluster:
. Verify your SNO kubeconfig is set:
+
[source,bash,role=execute]
----
# Your kubeconfig should be set from Step 2
echo $KUBECONFIG
oc whoami --show-server
----

=== Step 5: Understanding the Performance Operator Architecture

Your SNO cluster includes these pre-configured components:

[cols="1,3,1"]
|===
| Component | Purpose | Status

| *Node Tuning Operator*
| Manages TuneD profiles AND Performance Profiles. Handles CPU isolation, real-time kernels, and system-level tuning. Built-in since OpenShift 4.11+
| Built-in âœ…

| *SR-IOV Network Operator*
| Provides high-performance networking with direct hardware access for low-latency applications
| Pre-installed âœ…

| *OpenShift Virtualization*
| Enables running VMs with performance optimizations for low-latency virtualization scenarios
| Pre-installed âœ…
|===

=== Step 6: Final Verification

. Verify built-in Node Tuning Operator:
+
[source,bash,role=execute]
----
# Check Node Tuning Operator (built-in since 4.11+)
oc get tuned -n openshift-cluster-node-tuning-operator

# Verify Performance Profile CRD is available (managed by NTO)
oc get crd performanceprofiles.performance.openshift.io

# Check NTO pods are running
oc get pods -n openshift-cluster-node-tuning-operator
----

. Verify pre-installed operators:
+
[source,bash,role=execute]
----
# SR-IOV Network Operator
oc get csv -n openshift-sriov-network-operator

# OpenShift Virtualization
oc get csv -n openshift-cnv
----

. Check all operator pods are running:
+
[source,bash,role=execute]
----
# All operator namespaces
oc get pods -n openshift-cnv --field-selector=status.phase=Running | wc -l
oc get pods -n openshift-sriov-network-operator --field-selector=status.phase=Running | wc -l
oc get pods -n openshift-cluster-node-tuning-operator --field-selector=status.phase=Running | wc -l
----

[TIP]
====
**Expected Results:**

* âœ… Node Tuning Operator: TuneD pods running, PerformanceProfile CRD available
* âœ… SR-IOV Network Operator: CSV in "Succeeded" phase
* âœ… OpenShift Virtualization: CSV in "Succeeded" phase, HyperConverged instance ready

If any operator is not ready, wait 5-10 minutes for initialization to complete.
====

== Deep Dive: Manual RHACM-ArgoCD Setup (Reference)

[NOTE]
====
**This section is for advanced reference only.** Your workshop environment is already configured. This detailed walkthrough shows how the RHACM-ArgoCD integration was set up, which is useful for:

* Understanding the underlying architecture
* Setting up your own multi-cluster environments
* Troubleshooting integration issues
====

=== Creating a ManagedClusterSet

A ManagedClusterSet groups clusters together for easier management and policy application.

. Create the ManagedClusterSet resource:
+
[source,yaml]
----
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSet
metadata:
  name: all-clusters
----

. Apply the resource:
+
[source,bash,role=execute]
----
cat > managedclusterset.yaml << EOF
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSet
metadata:
  name: all-clusters
EOF

oc apply -f managedclusterset.yaml
----

. Add your clusters to the ManagedClusterSet by labeling them:
+
[source,bash,role=execute]
----
oc label managedcluster local-cluster cluster.open-cluster-management.io/clusterset=all-clusters --overwrite
oc label managedcluster cluster-tln8k cluster.open-cluster-management.io/clusterset=all-clusters --overwrite
----

=== Setting up ArgoCD Integration

Integrating RHACM with ArgoCD enables multi-cluster GitOps deployments.

. Create a ManagedClusterSetBinding to bind the cluster set to the openshift-gitops namespace:
+
[source,bash,role=execute]
----
cat > managedclustersetbinding.yaml << EOF
apiVersion: cluster.open-cluster-management.io/v1beta2
kind: ManagedClusterSetBinding
metadata:
  name: all-clusters
  namespace: openshift-gitops
spec:
  clusterSet: all-clusters
EOF

oc apply -f managedclustersetbinding.yaml
----

. Create a Placement resource to define which clusters should receive applications:
+
[source,bash,role=execute]
----
cat > placement.yaml << EOF
apiVersion: cluster.open-cluster-management.io/v1beta1
kind: Placement
metadata:
  name: all-clusters
  namespace: openshift-gitops
spec:
  clusterSets:
    - all-clusters
EOF

oc apply -f placement.yaml
----

. Create the GitOpsCluster resource to complete the integration:
+
[source,bash,role=execute]
----
cat > gitopscluster.yaml << EOF
apiVersion: apps.open-cluster-management.io/v1beta1
kind: GitOpsCluster
metadata:
  name: gitops-cluster
  namespace: openshift-gitops
spec:
  argoServer:
    cluster: local-cluster
    argoNamespace: openshift-gitops
  placementRef:
    kind: Placement
    apiVersion: cluster.open-cluster-management.io/v1beta1
    name: all-clusters
EOF

oc apply -f gitopscluster.yaml
----

. Verify the placement decision:
+
[source,bash,role=execute]
----
oc get placementdecision -n openshift-gitops
oc get placementdecision all-clusters-decision-1 -n openshift-gitops -o yaml
----

=== Deploying OpenShift Virtualization via ArgoCD (Reference)

With RHACM and ArgoCD integrated, operators can be deployed to remote clusters using GitOps.

. First, verify that both clusters are available in ArgoCD:
+
[source,bash,role=execute]
----
oc get secrets -n openshift-gitops | grep cluster
----
+
You should see cluster secrets for both managed clusters.

. Create an ArgoCD Application to deploy the OpenShift Virtualization operator:
+
[source,bash,role=execute]
----
cat > openshift-virtualization-operator.yaml << EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: openshift-virtualization-operator
  namespace: openshift-gitops
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/tosin2013/low-latency-performance-workshop.git
    targetRevision: HEAD
    path: gitops/openshift-virtualization/operator/overlays/sno
  destination:
    server: https://api.cluster-tln8k.dynamic.redhatworkshops.io:6443
    namespace: openshift-cnv
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
EOF

oc apply -f openshift-virtualization-operator.yaml
----

. Create an ArgoCD Application to deploy the OpenShift Virtualization instance:
+
[source,bash,role=execute]
----
cat > openshift-virtualization-instance.yaml << EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: openshift-virtualization-instance
  namespace: openshift-gitops
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/tosin2013/low-latency-performance-workshop.git
    targetRevision: HEAD
    path: gitops/openshift-virtualization/instance
  destination:
    server: https://api.cluster-tln8k.dynamic.redhatworkshops.io:6443
    namespace: openshift-cnv
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - ServerSideApply=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  ignoreDifferences:
    - group: hco.kubevirt.io
      kind: HyperConverged
      jsonPointers:
        - /status
EOF

oc apply -f openshift-virtualization-instance.yaml
----

. Monitor the application deployment:
+
[source,bash,role=execute]
----
oc get applications.argoproj.io -n openshift-gitops
----
+
Wait for both applications to show "Synced" and "Healthy" status.

=== Step 5: Verify OpenShift Virtualization Deployment

. Log into the target cluster to verify the deployment:
+
[source,bash,role=execute]
----
oc login --token=<target-cluster-token> --server=https://api.cluster-tln8k.dynamic.redhatworkshops.io:6443
----

. Check the OpenShift Virtualization operator installation:
+
[source,bash,role=execute]
----
oc get csv -n openshift-cnv
----
+
You should see the kubevirt-hyperconverged-operator in "Succeeded" phase.

. Verify the HyperConverged instance:
+
[source,bash,role=execute]
----
oc get hyperconverged -n openshift-cnv
----

. Check all OpenShift Virtualization pods are running:
+
[source,bash,role=execute]
----
oc get pods -n openshift-cnv
----

. Verify the HyperConverged status:
+
[source,bash,role=execute]
----
oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o yaml | grep -A 10 "conditions:"
----
+
Look for conditions showing "Available: True" and "ReconcileComplete: True".

== Understanding the GitOps Configuration

=== OpenShift Virtualization Operator Configuration

The operator deployment uses a Single Node OpenShift (SNO) specific overlay that includes:

* *KVM Emulation*: Enabled for virtualization on SNO environments
* *Automatic Installation*: InstallPlanApproval set to Automatic
* *Stable Channel*: Uses the stable operator channel for production readiness

=== HyperConverged Instance Configuration

The HyperConverged instance is configured with:

* *Live Migration*: Optimized settings for cluster performance
* *Node Placement*: Configured for single-node deployment patterns

== Installing Required Operators for Low-Latency Workloads

[NOTE]
====
**OpenShift 4.19 Performance Architecture Update**

In OpenShift 4.19, the performance operator landscape has been simplified:

* **Node Tuning Operator**: Built-in to OpenShift since 4.11+ - handles both TuneD profiles AND Performance Profiles
* **Performance Addon Operator**: **DEPRECATED** - functionality moved to built-in Node Tuning Operator
* **SR-IOV Network Operator**: Still requires installation for high-performance networking

This means we only need to install the SR-IOV Network Operator via GitOps, as the performance profile functionality is already available through the built-in Node Tuning Operator.
====

=== Verifying Built-in Node Tuning Operator

The Node Tuning Operator is built into OpenShift 4.11+ and manages both TuneD daemon and Performance Profiles.

. Verify the Node Tuning Operator is available:
+
[source,bash,role=execute]
----
# Check built-in Node Tuning Operator
oc get tuned -n openshift-cluster-node-tuning-operator

# Verify Performance Profile CRD is available
oc get crd performanceprofiles.performance.openshift.io

# Check NTO pods are running
oc get pods -n openshift-cluster-node-tuning-operator
----

=== SR-IOV Network Operator

The SR-IOV Network Operator manages Single-Root I/O Virtualization for high-performance networking with direct hardware access. This operator still requires installation.

. Create an ArgoCD Application for the SR-IOV Network Operator:
+
[source,bash,role=execute]
----
cat > sriov-network-operator.yaml << EOF
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: sriov-network-operator
  namespace: openshift-gitops
spec:
  project: default
  source:
    repoURL: https://github.com/tosin2013/low-latency-performance-workshop.git
    targetRevision: main
    path: gitops/sriov-network-operator/overlays/sno
  destination:
    server: https://kubernetes.default.svc
    namespace: openshift-sriov-network-operator
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
EOF

oc apply -f sriov-network-operator.yaml
----

=== Verify Performance Operators Installation

. Check the status of all performance-related operators:
+
[source,bash,role=execute]
----
# Check built-in Node Tuning Operator (handles Performance Profiles)
oc get tuned -n openshift-cluster-node-tuning-operator

# Check SR-IOV Network Operator
oc get csv -n openshift-sriov-network-operator

# Check OpenShift Virtualization  
oc get csv -n openshift-cnv

# Verify operator pods are running
oc get pods -n openshift-cluster-node-tuning-operator
oc get pods -n openshift-sriov-network-operator
oc get pods -n openshift-cnv
----

. Confirm ArgoCD applications are synced:
+
[source,bash,role=execute]
----
oc get applications.argoproj.io -n openshift-gitops | grep -E "(sriov|virtualization)"
----
+
All applications should show Status: "Synced" and Health: "Healthy".

.*Why These Operators Are Essential*
[NOTE]
====
* *Node Tuning Operator*: Built-in to OpenShift 4.19 - enables system-level performance tuning AND Performance Profiles
* *SR-IOV Network Operator*: Required for high-performance networking in Module 5 (Low-Latency Virtualization)
* *OpenShift Virtualization*: Enables low-latency virtual machine scenarios

**Note**: Performance Addon Operator is deprecated in OpenShift 4.11+ - its functionality is now built into the Node Tuning Operator.

These components are prerequisites for the hands-on exercises in Modules 4, 5, and 6.
====

== Workshop Environment Verification

Before proceeding to the next module, let's verify that the complete environment is ready for performance testing.

=== Verify RHACM-ArgoCD Integration

. Check that all ArgoCD applications are synced and healthy:
+
[source,bash,role=execute]
----
oc get applications.argoproj.io -n openshift-gitops
----
+
All applications should show Status: "Synced" and Health: "Healthy".

. Verify cluster connectivity from ArgoCD:
+
[source,bash,role=execute]
----
# List all cluster secrets in ArgoCD
oc get secrets -n openshift-gitops -l argocd.argoproj.io/secret-type=cluster

# Check cluster connectivity
oc get secrets -n openshift-gitops -l argocd.argoproj.io/secret-type=cluster -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'
----

=== Verify Target Cluster Readiness

. Switch context to your target cluster:
+
[source,bash,role=execute]
----
# List available contexts
oc config get-contexts

# Switch to target cluster (replace with your cluster name)
oc config use-context <target-cluster-context>
----

. Verify OpenShift Virtualization is ready:
+
[source,bash,role=execute]
----
# Check operator status
oc get csv -n openshift-cnv | grep kubevirt

# Check HyperConverged status
oc get hco -n openshift-cnv

# Verify all virtualization pods are running
oc get pods -n openshift-cnv --field-selector=status.phase=Running | wc -l
----

. Test basic cluster functionality:
+
[source,bash,role=execute]
----
# Check node status
oc get nodes

# Check cluster operators
oc get co | grep -v "True.*False.*False"

# Verify you can create resources
oc new-project test-connectivity
oc delete project test-connectivity
----

== Module Summary

In this module, you have:

âœ… *Verified* your SNO cluster is imported into RHACM +
âœ… *Confirmed* pre-installed operators (OpenShift Virtualization, SR-IOV) are running +
âœ… *Verified* built-in Node Tuning Operator for Performance Profile management +
âœ… *Understood* the GitOps architecture used for operator deployment (reference material) +
âœ… *Prepared* for baseline testing and performance tuning

.*Key Takeaways*
* Your workshop SNO cluster comes **pre-configured** with all required operators
* RHACM provides centralized management for multiple OpenShift clusters
* GitOps enables declarative application deployment (shown as reference)
* **OpenShift 4.19+** has mature built-in performance capabilities via Node Tuning Operator
* The workshop uses a safety-first approach with dedicated target SNO clusters

.*Your Pre-Configured Workshop Environment*
* âœ… SNO cluster imported and managed by RHACM
* âœ… OpenShift Virtualization operator deployed and configured
* âœ… SR-IOV Network Operator installed for high-performance networking
* âœ… Built-in Node Tuning Operator ready for Performance Profile management
* âœ… Environment ready for baseline testing and performance tuning

.*Next Steps*
In Module 3, you will establish baseline performance metrics on your target cluster using industry-standard tools like kube-burner. This will provide quantitative measurements that serve as the foundation for measuring improvements in subsequent modules.

== Troubleshooting Common Issues

=== Operator CSV Not Showing "Succeeded"

If an operator CSV is still in "Installing" or "Pending" state:

[source,bash,role=execute]
----
# Check CSV status with more detail
oc get csv -n openshift-cnv -o yaml | grep -A 10 "phase:"

# Check for pod issues
oc get pods -n openshift-cnv --field-selector=status.phase!=Running

# Check operator logs
oc logs -n openshift-cnv deployment/hco-operator --tail=50
----

=== Cluster Not Showing in RHACM

If your SNO cluster doesn't appear in the managed clusters list:

[source,bash,role=execute]
----
# Check ManagedCluster status on hub
oc get managedclusters

# Check for import issues
oc describe managedcluster workshop-student1

# Verify klusterlet is running on SNO
KUBECONFIG=~/agnosticd-output/test-student1/low-latency-workshop-sno_test-student1_kubeconfig \
  oc get pods -n open-cluster-management-agent
----

=== OpenShift Virtualization Pods Not Starting

If virtualization pods fail to start:

[source,bash,role=execute]
----
# Check node resources
oc describe nodes

# Verify HyperConverged status
oc get hyperconverged -n openshift-cnv -o yaml | grep -A 20 "conditions:"

# Check for pod errors
oc get pods -n openshift-cnv --field-selector=status.phase!=Running
----

=== SR-IOV Operator Not Ready

If SR-IOV operator is not fully initialized:

[source,bash,role=execute]
----
# Check SR-IOV operator status
oc get csv -n openshift-sriov-network-operator

# Verify config daemon is running
oc get pods -n openshift-sriov-network-operator

# Check SriovNetworkNodeState (may be empty on virtual/cloud instances)
oc get sriovnetworknodestates -n openshift-sriov-network-operator
----

== Best Practices

=== For Your Own Environments

* Use ManagedClusterSets to logically group clusters
* Implement proper RBAC for ArgoCD applications
* Use placement rules to target specific cluster types
* Monitor operator health across all clusters
* Use environment-specific overlays (SNO vs multi-node)

=== Performance Operator Architecture (OpenShift 4.19+)

* **Node Tuning Operator** (Built-in): Manages TuneD profiles AND Performance Profiles
* **Performance Addon Operator** (Deprecated): Functionality moved to Node Tuning Operator
* **SR-IOV Network Operator** (Pre-installed): High-performance networking capabilities
* **OpenShift Virtualization** (Pre-installed): Low-latency virtualization platform

== Knowledge Check

. What is the purpose of a ManagedClusterSet in RHACM?
. Why was the Performance Addon Operator deprecated in OpenShift 4.11+?
. What performance capabilities are now built into the Node Tuning Operator?
. Which operators come pre-installed on your workshop SNO cluster?
. How would you verify that OpenShift Virtualization is fully initialized?

== Next Steps

In the next module, you will:

* Use kube-burner to establish baseline performance metrics
* Measure pod creation latency and cluster response times
* Analyze performance data to identify optimization opportunities
* Create a performance baseline document for comparison

The **pre-installed operators** verified in this module will be essential for subsequent modules:

* *Module 4*: Performance Profiles (built-in Node Tuning Operator) for CPU isolation and real-time kernels
* *Module 5*: SR-IOV networking + OpenShift Virtualization for high-performance VM networking
* *Module 6*: TuneD profiles (built-in Node Tuning Operator) for system-level performance optimization

== Additional Resources

* link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes[RHACM Documentation^]
* link:https://docs.openshift.com/gitops/latest/[OpenShift GitOps Documentation^]
* link:https://docs.openshift.com/container-platform/latest/virt/about-virt.html[OpenShift Virtualization Documentation^]
* link:https://github.com/tosin2013/low-latency-performance-workshop[Workshop Repository^]
