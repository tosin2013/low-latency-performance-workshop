#!/usr/bin/env python3
"""
Module 6: Comprehensive Workshop Validator
End-to-end validation of all workshop modules (3-6)

This script validates all performance optimizations across the entire
workshop, from baseline measurements through tuning, virtualization,
and monitoring.
"""

import subprocess
import json
import sys
import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime

# Color codes for terminal output
class Colors:
    RED = '\033[91m'
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    MAGENTA = '\033[95m'
    CYAN = '\033[96m'
    BOLD = '\033[1m'
    END = '\033[0m'

    @staticmethod
    def disable():
        """Disable colors for non-terminal output"""
        Colors.RED = Colors.GREEN = Colors.YELLOW = Colors.BLUE = ''
        Colors.MAGENTA = Colors.CYAN = Colors.BOLD = Colors.END = ''


class WorkshopValidator:
    """Validates entire workshop completion and performance"""
    
    def __init__(self, metrics_dir: str = "~/kube-burner-configs"):
        self.metrics_dir = Path(metrics_dir).expanduser()
        self.validation_results = {
            'module3_baseline': False,
            'module4_tuning': False,
            'module5_vmi': False,
            'module5_network': False,
            'cluster_health': False,
            'performance_improvement': False
        }
        self.metrics_found = {}
        
    def run_oc_command(self, cmd: List[str], timeout: int = 30) -> Tuple[bool, str]:
        """Run oc command and return success status and output"""
        try:
            result = subprocess.run(['oc'] + cmd, capture_output=True, text=True, timeout=timeout)
            return result.returncode == 0, result.stdout.strip()
        except Exception as e:
            return False, str(e)
    
    def print_header(self):
        """Print validation header"""
        print(f"\n{Colors.BOLD}{Colors.CYAN}{'='*70}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.CYAN}ðŸ” Module 6: Comprehensive Workshop Validator{Colors.END}")
        print(f"{Colors.BOLD}{Colors.CYAN}{'='*70}{Colors.END}\n")
        
        print(f"{Colors.BOLD}ðŸ“‹ What This Validator Checks:{Colors.END}")
        print(f"  1ï¸âƒ£  Module 3: Baseline performance metrics collected")
        print(f"  2ï¸âƒ£  Module 4: Performance tuning applied (optional)")
        print(f"  3ï¸âƒ£  Module 5: VMI performance testing completed")
        print(f"  4ï¸âƒ£  Module 5: Network policy testing completed")
        print(f"  5ï¸âƒ£  Cluster health and configuration")
        print(f"  6ï¸âƒ£  Overall performance improvements\n")
    
    def validate_module3_baseline(self) -> Dict:
        """Validate Module 3 baseline metrics exist"""
        print(f"{Colors.BOLD}{Colors.BLUE}1ï¸âƒ£  Validating Module 3: Baseline Metrics{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        baseline_dir = self.metrics_dir / "collected-metrics"
        
        if not baseline_dir.exists():
            print(f"{Colors.RED}âŒ Baseline metrics not found{Colors.END}")
            print(f"{Colors.YELLOW}ðŸ’¡ Run baseline tests in Module 3{Colors.END}\n")
            return {'status': 'missing', 'path': None}
        
        # Check for expected metric files
        expected_files = [
            "podLatencyQuantilesMeasurement-baseline-workload.json",
            "podLatencyMeasurement-baseline-workload.json"
        ]
        
        found_files = []
        for file in expected_files:
            if (baseline_dir / file).exists():
                found_files.append(file)
        
        if found_files:
            print(f"{Colors.GREEN}âœ… Baseline metrics found: {len(found_files)}/{len(expected_files)} files{Colors.END}")
            for file in found_files:
                print(f"  â€¢ {file}")
            print()
            self.validation_results['module3_baseline'] = True
            self.metrics_found['baseline'] = True
            return {'status': 'complete', 'path': baseline_dir, 'files': found_files}
        else:
            print(f"{Colors.YELLOW}âš ï¸  Baseline metrics incomplete{Colors.END}\n")
            return {'status': 'incomplete', 'path': baseline_dir}
    
    def validate_module4_tuning(self) -> Dict:
        """Validate Module 4 performance tuning"""
        print(f"{Colors.BOLD}{Colors.BLUE}2ï¸âƒ£  Validating Module 4: Performance Tuning{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        # Check for performance profile
        success, output = self.run_oc_command(['get', 'performanceprofile', '-o', 'json'])
        
        has_profile = False
        if success and output:
            try:
                profiles_data = json.loads(output)
                profiles = profiles_data.get('items', [])
                if profiles:
                    profile = profiles[0]
                    profile_name = profile['metadata']['name']
                    print(f"{Colors.GREEN}âœ… Performance Profile found: {profile_name}{Colors.END}")
                    
                    # Show key settings
                    spec = profile['spec']
                    cpu_config = spec.get('cpu', {})
                    print(f"  â€¢ Reserved CPUs: {cpu_config.get('reserved', 'N/A')}")
                    print(f"  â€¢ Isolated CPUs: {cpu_config.get('isolated', 'N/A')}")
                    
                    hugepages = spec.get('hugepages', {})
                    if hugepages:
                        pages = hugepages.get('pages', [])
                        for page in pages:
                            print(f"  â€¢ HugePages: {page.get('count', 0)} x {page.get('size', 'N/A')}")
                    
                    has_profile = True
            except json.JSONDecodeError:
                pass
        
        # Check for tuned metrics
        tuned_dir = self.metrics_dir / "collected-metrics-tuned"
        has_tuned_metrics = tuned_dir.exists()
        
        if has_profile and has_tuned_metrics:
            print(f"{Colors.GREEN}âœ… Tuned performance metrics found{Colors.END}\n")
            self.validation_results['module4_tuning'] = True
            self.metrics_found['tuned'] = True
            return {'status': 'complete', 'profile': True, 'metrics': True}
        elif has_profile:
            print(f"{Colors.YELLOW}âš ï¸  Performance Profile exists but no tuned metrics{Colors.END}")
            print(f"{Colors.CYAN}ðŸ’¡ Run performance tests in Module 4{Colors.END}\n")
            return {'status': 'partial', 'profile': True, 'metrics': False}
        else:
            print(f"{Colors.CYAN}â„¹ï¸  Module 4 not completed (optional){Colors.END}")
            print(f"{Colors.CYAN}This module is optional but recommended for best performance{Colors.END}\n")
            return {'status': 'skipped', 'profile': False, 'metrics': False}
    
    def validate_module5_vmi(self) -> Dict:
        """Validate Module 5 VMI testing"""
        print(f"{Colors.BOLD}{Colors.BLUE}3ï¸âƒ£  Validating Module 5: VMI Performance{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        # Check for VMI metrics
        vmi_dir = self.metrics_dir / "collected-metrics-vmi"
        
        if not vmi_dir.exists():
            print(f"{Colors.YELLOW}âš ï¸  VMI metrics not found{Colors.END}")
            print(f"{Colors.CYAN}ðŸ’¡ Run VMI tests in Module 5{Colors.END}\n")
            return {'status': 'missing', 'path': None}
        
        # Check for VMI-specific files
        vmi_files = list(vmi_dir.glob("vmiLatency*.json"))
        
        if vmi_files:
            print(f"{Colors.GREEN}âœ… VMI metrics found: {len(vmi_files)} file(s){Colors.END}")
            for file in vmi_files:
                print(f"  â€¢ {file.name}")
            print()
            
            # Check for running VMIs
            success, output = self.run_oc_command(['get', 'vmi', '--all-namespaces', '-o', 'json'])
            if success and output:
                try:
                    vmis_data = json.loads(output)
                    vmis = vmis_data.get('items', [])
                    running_vmis = [v for v in vmis if v.get('status', {}).get('phase') == 'Running']
                    
                    if running_vmis:
                        print(f"{Colors.GREEN}âœ… Found {len(running_vmis)} running VMI(s){Colors.END}\n")
                    else:
                        print(f"{Colors.CYAN}â„¹ï¸  No running VMIs (tests may have completed){Colors.END}\n")
                except json.JSONDecodeError:
                    pass
            
            self.validation_results['module5_vmi'] = True
            self.metrics_found['vmi'] = True
            return {'status': 'complete', 'path': vmi_dir, 'files': vmi_files}
        else:
            print(f"{Colors.YELLOW}âš ï¸  VMI metrics incomplete{Colors.END}\n")
            return {'status': 'incomplete', 'path': vmi_dir}
    
    def validate_module5_network(self) -> Dict:
        """Validate Module 5 network policy testing"""
        print(f"{Colors.BOLD}{Colors.BLUE}4ï¸âƒ£  Validating Module 5: Network Policy Testing{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        # Check for network policy metrics
        netpol_files = list(self.metrics_dir.glob("**/netpolLatency*.json"))
        
        if netpol_files:
            print(f"{Colors.GREEN}âœ… Network policy metrics found: {len(netpol_files)} file(s){Colors.END}")
            for file in netpol_files:
                print(f"  â€¢ {file.name}")
            print()
            self.validation_results['module5_network'] = True
            return {'status': 'complete', 'files': netpol_files}
        else:
            print(f"{Colors.CYAN}â„¹ï¸  Network policy metrics not found (optional){Colors.END}\n")
            return {'status': 'skipped'}
    
    def validate_cluster_health(self) -> Dict:
        """Validate overall cluster health"""
        print(f"{Colors.BOLD}{Colors.BLUE}5ï¸âƒ£  Validating Cluster Health{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        health_checks = {
            'nodes': False,
            'operators': False,
            'monitoring': False
        }
        
        # Check nodes
        success, output = self.run_oc_command(['get', 'nodes', '-o', 'json'])
        if success and output:
            try:
                nodes_data = json.loads(output)
                nodes = nodes_data.get('items', [])
                ready_nodes = sum(1 for n in nodes 
                                 if any(c.get('type') == 'Ready' and c.get('status') == 'True' 
                                       for c in n.get('status', {}).get('conditions', [])))
                
                print(f"{Colors.GREEN}âœ… Nodes: {ready_nodes}/{len(nodes)} ready{Colors.END}")
                health_checks['nodes'] = ready_nodes == len(nodes)
            except json.JSONDecodeError:
                print(f"{Colors.RED}âŒ Failed to parse node data{Colors.END}")
        
        # Check cluster operators
        success, output = self.run_oc_command(['get', 'clusteroperator', '-o', 'json'])
        if success and output:
            try:
                ops_data = json.loads(output)
                operators = ops_data.get('items', [])
                available_ops = sum(1 for op in operators
                                   if any(c.get('type') == 'Available' and c.get('status') == 'True'
                                         for c in op.get('status', {}).get('conditions', [])))
                
                print(f"{Colors.GREEN}âœ… Cluster Operators: {available_ops}/{len(operators)} available{Colors.END}")
                health_checks['operators'] = available_ops == len(operators)
            except json.JSONDecodeError:
                print(f"{Colors.RED}âŒ Failed to parse operator data{Colors.END}")
        
        # Check monitoring
        success, output = self.run_oc_command(['get', 'pods', '-n', 'openshift-monitoring', '-o', 'json'])
        if success and output:
            try:
                pods_data = json.loads(output)
                pods = pods_data.get('items', [])
                running_pods = sum(1 for p in pods if p.get('status', {}).get('phase') == 'Running')
                
                print(f"{Colors.GREEN}âœ… Monitoring: {running_pods}/{len(pods)} pods running{Colors.END}")
                health_checks['monitoring'] = running_pods > 0
            except json.JSONDecodeError:
                print(f"{Colors.YELLOW}âš ï¸  Monitoring status unknown{Colors.END}")
        
        print()
        
        all_healthy = all(health_checks.values())
        self.validation_results['cluster_health'] = all_healthy
        
        return {'status': 'healthy' if all_healthy else 'degraded', 'checks': health_checks}
    
    def calculate_improvements(self) -> Dict:
        """Calculate performance improvements across modules"""
        print(f"{Colors.BOLD}{Colors.BLUE}6ï¸âƒ£  Calculating Performance Improvements{Colors.END}")
        print(f"{Colors.BLUE}{'â”€'*70}{Colors.END}\n")
        
        improvements = {}
        
        # This is a simplified calculation - in reality, you'd parse the JSON files
        if self.metrics_found.get('baseline') and self.metrics_found.get('tuned'):
            print(f"{Colors.GREEN}âœ… Can calculate baseline vs tuned improvement{Colors.END}")
            print(f"{Colors.CYAN}ðŸ’¡ Expected: 50-70% reduction in P99 latency{Colors.END}")
            improvements['baseline_to_tuned'] = True
            self.validation_results['performance_improvement'] = True
        
        if self.metrics_found.get('vmi'):
            print(f"{Colors.GREEN}âœ… VMI performance data available{Colors.END}")
            print(f"{Colors.CYAN}ðŸ’¡ Expected: VMI startup 60-90 seconds{Colors.END}")
            improvements['vmi_performance'] = True
        
        print()
        
        return improvements

    def print_summary(self):
        """Print comprehensive validation summary"""
        print(f"{Colors.BOLD}{Colors.GREEN}{'='*70}{Colors.END}")
        print(f"{Colors.BOLD}{Colors.GREEN}ðŸ“Š Comprehensive Validation Summary{Colors.END}")
        print(f"{Colors.BOLD}{Colors.GREEN}{'='*70}{Colors.END}\n")

        # Count completed validations
        completed = sum(1 for v in self.validation_results.values() if v)
        total = len(self.validation_results)

        print(f"{Colors.BOLD}Validation Results: {completed}/{total} checks passed{Colors.END}\n")

        # Module-by-module status
        print(f"{Colors.BOLD}Module Status:{Colors.END}")

        status_icon = lambda x: f"{Colors.GREEN}âœ…{Colors.END}" if x else f"{Colors.RED}âŒ{Colors.END}"

        print(f"  {status_icon(self.validation_results['module3_baseline'])} Module 3: Baseline Metrics")
        print(f"  {status_icon(self.validation_results['module4_tuning'])} Module 4: Performance Tuning (optional)")
        print(f"  {status_icon(self.validation_results['module5_vmi'])} Module 5: VMI Testing")
        print(f"  {status_icon(self.validation_results['module5_network'])} Module 5: Network Policy Testing")
        print(f"  {status_icon(self.validation_results['cluster_health'])} Cluster Health")
        print(f"  {status_icon(self.validation_results['performance_improvement'])} Performance Improvements")
        print()

        # Overall status
        if completed == total:
            print(f"{Colors.GREEN}{Colors.BOLD}ðŸŽ‰ ALL VALIDATIONS PASSED!{Colors.END}")
            print(f"{Colors.GREEN}The workshop is complete and all optimizations are validated.{Colors.END}\n")
        elif completed >= total * 0.7:
            print(f"{Colors.YELLOW}{Colors.BOLD}âš ï¸  MOST VALIDATIONS PASSED{Colors.END}")
            print(f"{Colors.YELLOW}Some optional components are missing but core workshop is complete.{Colors.END}\n")
        else:
            print(f"{Colors.RED}{Colors.BOLD}âŒ VALIDATION INCOMPLETE{Colors.END}")
            print(f"{Colors.RED}Please complete the missing modules before proceeding.{Colors.END}\n")

        # Next steps
        print(f"{Colors.BOLD}ðŸ“š Next Steps:{Colors.END}")

        if not self.validation_results['module3_baseline']:
            print(f"  â€¢ Complete Module 3: Run baseline performance tests")

        if not self.validation_results['module4_tuning']:
            print(f"  â€¢ Optional: Complete Module 4 for performance tuning")

        if not self.validation_results['module5_vmi']:
            print(f"  â€¢ Complete Module 5: Run VMI performance tests")

        if not self.validation_results['module5_network']:
            print(f"  â€¢ Optional: Run network policy tests in Module 5")

        if completed == total:
            print(f"  â€¢ Review performance summary with {Colors.CYAN}module06-workshop-summary.py{Colors.END}")
            print(f"  â€¢ Set up continuous monitoring")
            print(f"  â€¢ Deploy to production with confidence!")

        print()

    def generate_report(self, output_file: Optional[str] = None):
        """Generate detailed validation report"""
        if not output_file:
            output_file = f"workshop-validation-{datetime.now().strftime('%Y%m%d-%H%M%S')}.md"

        report_lines = [
            f"# Workshop Validation Report",
            f"",
            f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"## Validation Summary",
            f"",
            f"| Module | Component | Status |",
            f"|--------|-----------|--------|",
            f"| Module 3 | Baseline Metrics | {'âœ… Complete' if self.validation_results['module3_baseline'] else 'âŒ Missing'} |",
            f"| Module 4 | Performance Tuning | {'âœ… Complete' if self.validation_results['module4_tuning'] else 'âš ï¸ Optional'} |",
            f"| Module 5 | VMI Testing | {'âœ… Complete' if self.validation_results['module5_vmi'] else 'âŒ Missing'} |",
            f"| Module 5 | Network Testing | {'âœ… Complete' if self.validation_results['module5_network'] else 'âš ï¸ Optional'} |",
            f"| Cluster | Health Check | {'âœ… Healthy' if self.validation_results['cluster_health'] else 'âŒ Issues'} |",
            f"| Overall | Performance | {'âœ… Improved' if self.validation_results['performance_improvement'] else 'âš ï¸ N/A'} |",
            f"",
            f"## Metrics Found",
            f"",
        ]

        for metric_type, found in self.metrics_found.items():
            report_lines.append(f"- **{metric_type.capitalize()}**: {'âœ… Found' if found else 'âŒ Not found'}")

        report_lines.extend([
            f"",
            f"## Recommendations",
            f"",
        ])

        if not self.validation_results['module3_baseline']:
            report_lines.append(f"- âš ï¸ **Critical**: Complete Module 3 baseline testing")

        if not self.validation_results['module4_tuning']:
            report_lines.append(f"- ðŸ’¡ **Optional**: Consider Module 4 for 50-70% performance improvement")

        if not self.validation_results['module5_vmi']:
            report_lines.append(f"- âš ï¸ **Important**: Complete Module 5 VMI testing")

        if all(self.validation_results.values()):
            report_lines.append(f"- ðŸŽ‰ **Excellent**: All validations passed! Workshop complete.")

        report_lines.extend([
            f"",
            f"## Next Steps",
            f"",
            f"1. Review detailed performance analysis",
            f"2. Set up continuous monitoring",
            f"3. Configure alerting for regressions",
            f"4. Document baseline and optimized performance",
            f"5. Deploy optimizations to production",
            f"",
        ])

        report_content = "\n".join(report_lines)

        try:
            with open(output_file, 'w') as f:
                f.write(report_content)
            print(f"{Colors.GREEN}âœ… Validation report saved: {output_file}{Colors.END}\n")
        except Exception as e:
            print(f"{Colors.RED}âŒ Failed to save report: {e}{Colors.END}\n")

    def run_validation(self, generate_report: bool = False):
        """Run complete validation"""
        self.print_header()

        # Run all validations
        self.validate_module3_baseline()
        self.validate_module4_tuning()
        self.validate_module5_vmi()
        self.validate_module5_network()
        self.validate_cluster_health()
        self.calculate_improvements()

        # Print summary
        self.print_summary()

        # Generate report if requested
        if generate_report:
            self.generate_report()


def main():
    parser = argparse.ArgumentParser(
        description="Module 6: Comprehensive Workshop Validator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
This script validates the entire workshop completion:

  âœ“ Module 3: Baseline performance metrics
  âœ“ Module 4: Performance tuning (optional)
  âœ“ Module 5: VMI and network testing
  âœ“ Cluster health and configuration
  âœ“ Overall performance improvements

Examples:
  # Run comprehensive validation
  python3 module06-comprehensive-validator.py

  # Generate validation report
  python3 module06-comprehensive-validator.py --report

  # Specify custom metrics directory
  python3 module06-comprehensive-validator.py --metrics-dir /path/to/metrics

  # Disable colored output
  python3 module06-comprehensive-validator.py --no-color

Educational Focus:
  This script validates that all workshop modules are complete
  and performance optimizations are working correctly.
        """
    )

    parser.add_argument(
        "--metrics-dir",
        default="~/kube-burner-configs",
        help="Directory containing performance metrics (default: ~/kube-burner-configs)"
    )

    parser.add_argument(
        "--report",
        action="store_true",
        help="Generate detailed validation report"
    )

    parser.add_argument(
        "--no-color",
        action="store_true",
        help="Disable colored output"
    )

    args = parser.parse_args()

    # Disable colors if requested or not in a TTY
    if args.no_color or not sys.stdout.isatty():
        Colors.disable()

    # Create validator and run
    validator = WorkshopValidator(metrics_dir=args.metrics_dir)
    validator.run_validation(generate_report=args.report)

    # Exit with appropriate code
    completed = sum(1 for v in validator.validation_results.values() if v)
    total = len(validator.validation_results)

    if completed == total:
        sys.exit(0)  # All validations passed
    elif completed >= total * 0.7:
        sys.exit(0)  # Most validations passed (optional items missing)
    else:
        sys.exit(1)  # Critical validations failed


if __name__ == "__main__":
    main()


